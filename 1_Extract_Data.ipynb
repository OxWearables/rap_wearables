{"cells":[{"cell_type":"markdown","metadata":{},"source":["# Extracting data using the UKB RAP\n","\n","## Introduction\n","In this notebook, we will dispense data for analysis using the RAP. \n","\n","## About this notebook\n","\n","This is a Python notebook.\n","\n","Data on the RAP is dispensed using Spark (a data processing framework well-suited to big data which works by distributing data and data processing over several machines). We will access the data using [pyspark](https://pypi.org/project/pyspark/), a Python interface to Spark.\n","\n","Here, we will also do some manipulation of the data using pyspark. There are alternatives: you could convert earlier to a [pandas](https://pandas.pydata.org/) or [koalas](https://koalas.readthedocs.io/en/latest/) data frame and manipulate those. Be aware that working in pandas no longer takes advantage of the distributed aspect of Spark, so you may run into issues with the size of the data. koalas looks and feels like pandas but makes use of the distributed aspect of Spark.\n","\n","## How to run this notebook\n","\n","This notebook should be run in a *Spark in JupyterLab* session. Read how to set it up [here](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data).\n","\n","## Set up the session\n","\n","We load modules we'll use, and set up the Spark session:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["import pyspark\n","import dxpy # tools starting with 'dx' are from the DNANexus ecosystem\n","import dxdata\n","from pyspark.sql.functions import when, concat_ws\n","from re import sub"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["sc = pyspark.SparkContext()\n","spark = pyspark.sql.SparkSession(sc)"]},{"cell_type":"markdown","metadata":{},"source":["## Dispense a dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dispensed_database = dxpy.find_one_data_object(\n","    classname=\"database\", \n","    name=\"app*\", \n","    folder=\"/\", \n","    name_mode=\"glob\", \n","    describe=True)\n","dispensed_database_name = dispensed_database[\"describe\"][\"name\"]\n","\n","dispensed_dataset = dxpy.find_one_data_object(\n","    typename=\"Dataset\", \n","    name=\"app*.dataset\", \n","    folder=\"/\", \n","    name_mode=\"glob\")\n","dispensed_dataset_id = dispensed_dataset[\"id\"]"]},{"cell_type":"markdown","metadata":{},"source":["## Load dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["dataset = dxdata.load_dataset(id=dispensed_dataset_id)"]},{"cell_type":"markdown","metadata":{},"source":["## Tabular participant data\n","\n","We first load the \"straightforward\" tabular participant data:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["participant = dataset[\"participant\"]"]},{"cell_type":"markdown","metadata":{},"source":["Next, we'll load a list of fields we might want for our analysis. We use this utility function:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def load_column_list(file_name):\n","    \"\"\"Load list of UK Biobank column IDs from file\n","    Column IDs can be obtained from the UK Biobank showcase: https://biobank.ndph.ox.ac.uk/showcase/index.cgi\n","    e.g. 90012 refers to the recommended variable for accelerometer measured physical activity\n","        https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=90012\n","        \n","    This function is due to Aiden Doherty.\n","\n","    :param str file_name: Name of file listing UK Biobank column IDs\n","    :return: list of IDs\n","    :rtype: list\n","    :Example:\n","    >>> load_column_list(\"field_list.txt\")\n","    [\"31\", \"34\", \"52\" ...]\n","    \"\"\"\n","    \n","    column_IDs = []\n","    \n","    with open(file_name) as f:\n","            for line in f:\n","                li = line.strip()\n","                if \"#\" in li:\n","                    li = li.split(\"#\")[0].strip()\n","                if not li.startswith(\"#\") and not li==\"\":\n","                    column_IDs += [li]\n","    return column_IDs"]},{"cell_type":"markdown","metadata":{},"source":["We read in the list:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["column_list = load_column_list(\"candidate_field_list.txt\") # This is a text file listing the field IDs we'll use\n","# You should change this file location as appropriate if you change the list of fields you want."]},{"cell_type":"markdown","metadata":{},"source":["Next, we'll use these field numbers to work out the [field *names*](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database) we need.\n","\n","In particular, UK Biobank fields sometimes have *instance* and *array* indices, which are [indicated in the field names](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database). Instances refer to different occasions when participants were measured (e.g. different visits to the assessment centre). For most instanced variables, [instance 0 is baseline, instance 1 is a resurvey among a limited number of participants, instance 2 is the imaging visit, and instance 3 is the first repeat imaging visit](https://biobank.ndph.ox.ac.uk/ukb/instance.cgi?id=2). At each instance after baseline, only a subset of participants were measured. Array indices are used where variables have multiple items e.g. participants gave multiple answers (e.g. employment history) or had multiple measurements on a single occasion (e.g. blood pressure).\n","\n","We write utility functions to get names for a particular field and instance. These functions are based on [functions from DNANexus available on GitHub here](https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb). "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def fields_for_id(field_id):\n","    from distutils.version import LooseVersion\n","    field_id = str(field_id)\n","    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n","    return sorted(fields, key=lambda f: LooseVersion(f.name))\n","\n","def field_names_for_id(field_id):\n","    return [f.name for f in fields_for_id(field_id)] # Returns all field names for a given field ID (i.e. all instance and array indices)\n","\n","def field_names_for_id_instanced(field_id, instance=\"i0\", include_non_instanced=True):\n","    candidate_fields = [f.name for f in fields_for_id(field_id)]\n","    return_fields = [f for f in candidate_fields if instance in f]\n","    if (include_non_instanced): # This means fields without instancing (e.g. sex) will be included, and defaults to true\n","        return_fields += [f for f in candidate_fields if \"_i\" not in f]\n","    return return_fields"]},{"cell_type":"markdown","metadata":{},"source":["In this analysis, we will take non-accelerometer participant data from baseline. This means we need variables from instance 0 or with no instancing (some variables, like sex, have one value across all visits; the accelerometer variables are also not instanced):"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["field_list = [\"eid\"]\n","for col in column_list:\n","    field_list += field_names_for_id_instanced(col, instance=\"i0\")"]},{"cell_type":"markdown","metadata":{},"source":["An aside: as accelerometer wear occurred several years after baseline assessment, and some participants have undergone repeat assessments between baseline assessment and accelerometer wear, so we might alternatively use data from the most recent assessment before accelerometer wear for covariate adjustment. Given only a small proportion of participants have a second visit before accelerometer wear, for simplicity we ignore that here. However, we will add a couple of fields on self-reported conditions from repeat assessments, so we can more rigorously account for prevalent disease."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["field_list += [\"p6150_i1\", \"p6150_i2\", \"p53_i1\", \"p53_i2\"] # we'll take a couple of extra fields to allow us to\n","# more rigorously exclude people with prevalent disease"]},{"cell_type":"markdown","metadata":{},"source":["If we just input the field list like this, we'll retrieve fields with their UK Biobank field names (format: [pX_iX_aX](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data#database-columns)). We can set up some aliases with more human readable names:\n","\n","[We use two different approaches here to illustrate them- you can use either, neither (i.e. use the fields with their original field names), or a mix.]"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# We'll handcraft some of the names to slot neatly into later code\n","field_list_aliases = {\n","    \"eid\" : \"eid\", \n","    \"p31\" :  \"sex\",\n","    \"p52\" : \"month_birth\", \n","    \"p34\" : \"year_birth\", \n","    \"p54_i0\" :\"ukb_assess_cent\", \n","    \"p21000_i0\" : \"ethnicity_raw\",\n","    \"p189\" : \"tdi_raw\",\n","    \"p6138_i0\" : \"qualif_raw\",\n","    \"p845_i0\" : \"age_education_raw\",\n","    \"p20116_i0\" :\"smoking_raw\",\n","    \"p1558_i0\" :\"alcohol_raw\", \n","    \"p21001_i0\" : \"BMI_raw\", \n","    \"p191\": \"date_lost_followup\",\n","    \"p6150_i0\": \"self_report_cvd_baseline\", \n","    \"p6150_i1\": \"self_report_cvd_inst_1\", \n","    \"p6150_i2\": \"self_report_cvd_inst_2\", \n","    \"p53_i0\": \"date_baseline\", \n","    \"p53_i1\": \"date_inst_1\", \n","    \"p53_i2\": \"date_inst_2\", \n","    \"p90016\" : \"quality_good_calibration\",\n","    \"p90183\" : \"clips_before_cal\",\n","    \"p90185\" : \"clips_after_cal\", \n","    \"p90187\" : \"total_reads\",\n","    \"p90015\" :\"quality_good_wear_time\",\n","    \"p90012\": \"overall_activity\", \n","    \"p90011\": \"date_end_accel\"\n","    }\n","\n","# We'll then auto-add names for other fields \n","dataset_fields = [participant.find_field(name=x) for x in field_list] # get the full field info, not just the name\n","for field in dataset_fields:\n","    if field.name not in field_list_aliases.keys():\n","        field_list_aliases[field.name] = field.title"]},{"cell_type":"markdown","metadata":{},"source":["We retrieve relevant data:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["participant_data = participant.retrieve_fields(names=field_list, engine=dxdata.connect(), coding_values=\"replace\", column_aliases=field_list_aliases)"]},{"cell_type":"markdown","metadata":{},"source":["Setting coding values to \"replace\" in this function means we get variables' values (rather than the coded format they are stored in)."]},{"cell_type":"markdown","metadata":{},"source":["As we will analyse only those participants with accelerometer data, we drop participants without accelerometer data:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["participant_wacc_data = participant_data.filter(participant_data.date_end_accel.isNotNull()) # the end time of accelerometer wear should be present for all\n","# participants with accelerometer data "]},{"cell_type":"markdown","metadata":{},"source":["We now write the data out as a csv file. \n","\n","Variables for which the value is itself an array (the usual format where the participant could tick several boxes) cannot be written to csv. We convert the affected variables to string variables: "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["arrayed_cols = [\"qualif_raw\", \"self_report_cvd_baseline\", \"self_report_cvd_inst_1\", \"self_report_cvd_inst_2\"]\n","for col in arrayed_cols:\n","    participant_wacc_data = participant_wacc_data.withColumn(col, concat_ws(\"|\", col))"]},{"cell_type":"markdown","metadata":{},"source":["Spark stores and works with the data in several parts, so we coalesce them into one part:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["participant_wacc_data = participant_wacc_data.coalesce(1)"]},{"cell_type":"markdown","metadata":{},"source":["We then set options and write to a csv file:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["participant_wacc_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"participant_wacc_data\")"]},{"cell_type":"markdown","metadata":{},"source":["Currently it is saved only in this session, so we need to upload to storage for reuse. As it was written by Spark, it is also stored in the \"hdfs\" (Hadoop Distributed File System), in a structured folder, so we first get it out of that and into \"standard\" storage:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%bash\n","hdfs dfs -copyToLocal /user/root/participant_wacc_data participant_wacc_data # this gets it out of the hdfs\n","dx upload participant_wacc_data/*.csv --dest users/Rosemary_Walmsley32/data/participant_wacc_data.csv # this uploads to the permanent storage on the RAP\n","# Note that Spark/hdfs ends up creating a folder containing the file, even though we coalesced to a single part. \n","# That's why the second line of code here looks for any csv file in the relevant folder."]},{"cell_type":"markdown","metadata":{},"source":["## Hospital data"]},{"cell_type":"markdown","metadata":{},"source":["There are several sources of data on health and disease outcomes in the UK Biobank dataset. These include data from inpatient hospital admissions (sometimes referred to as \"Hospital Episode Statistics\"), death registry data, cancer registry data, primary care data on some participants/for some purposes, and some outcomes added to the tabular data (including the \"algorithmically defined outcomes\" identifying particular diseases across different data sources). \n","\n","For this analysis (following [1](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003487)), we will use hospital inpatient data on diagnoses and death data.\n","\n","We will use two tables from the hospital inpatient data: hesin and hesin_diag. An entry in hesin is associated with a single hospital *episode* (generally: [\"a continuous period\n","of admitted patient care administered under one consultant within that one hospital provider\"](https://biobank.ndph.ox.ac.uk/ukb/ukb/docs/HospitalEpisodeStatistics.pdf)), and provides information such as dates associated with the episode. Entries in hesin_diag are associated with a particular diagnosis within a particular episode.\n","\n","In this section, we will access, process, and save it.\n","\n","We load the data: "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hesin = dataset[\"hesin\"]\n","hesin_diag = dataset[\"hesin_diag\"]\n","\n","hesin_data = hesin.retrieve_fields(engine=dxdata.connect())\n","hesin_diag_data = hesin_diag.retrieve_fields(engine=dxdata.connect())"]},{"cell_type":"markdown","metadata":{},"source":["A small proportion of episodes are missing an episode start date but have different dates associated with the episode. In an optional additional step, we make a new \"dateepiimp\" column which takes the value of \"epistart\", but when that is missing imputes them with \"disdate\" [NOTE - 2022-10-24, UK Biobank's own protocol when producing derived fields is to impute with admidate, epiend and disdate (see https://biobank.ndph.ox.ac.uk/showcase/ukb/docs/first_occurrences_outcomes.pdf, page 8). This notebook will be updated to match.] :"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hesin_data = hesin_data.withColumn(\"dateepiimp\",\n","                                   when(hesin_data[\"epistart\"].isNotNull(), hesin_data[\"epistart\"]).otherwise(hesin_data[\"disdate\"]))"]},{"cell_type":"markdown","metadata":{},"source":[" To associate dates with diagnoses, we need to join hesin and hesin_diag. We join on \"eid\" (participant ID), \"ins_index\" (the hospital episode in the data) and \"dnx_hesin_id\" (which is a combination of the participant ID and the instance index and so can be used to join uniquely - we join on all three just to avoid duplicating columns). "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hes_data = hesin_data.join(\n","    hesin_diag_data, \n","    [\"eid\", \"ins_index\", \"dnx_hesin_id\"],\n","    \"left_outer\"\n",")"]},{"cell_type":"markdown","metadata":{},"source":["We keep only those participants with accelerometry data:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["wacc_data =  participant_wacc_data.select(\"eid\").rdd.flatMap(lambda x: x).collect()\n","hes_wacc_data = hes_data.filter(hes_data.eid.isin(wacc_data))"]},{"cell_type":"markdown","metadata":{},"source":["As for the tabular participant data, we coalesce to a single part and write out to a csv file:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["hes_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"hes_wacc_data\")"]},{"cell_type":"markdown","metadata":{},"source":["And again upload it to storage so we can reuse it: "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%bash\n","hdfs dfs -copyToLocal /user/root/hes_wacc_data hes_wacc_data\n","dx upload hes_wacc_data/*.csv --dest users/Rosemary_Walmsley32/data/hes_wacc_data.csv"]},{"cell_type":"markdown","metadata":{},"source":["# Death data"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["death = dataset[\"death\"]\n","death_cause = dataset[\"death_cause\"]\n","\n","death_data = death.retrieve_fields(engine=dxdata.connect())\n","death_cause_data = death_cause.retrieve_fields(engine=dxdata.connect())"]},{"cell_type":"markdown","metadata":{},"source":["To reduce the size of the dataset, we keep only those participants with accelerometry data:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["death_wacc_data = death_data.filter(death_data.eid.isin(wacc_data))\n","death_cause_wacc_data = death_cause_data.filter(death_cause_data.eid.isin(wacc_data))"]},{"cell_type":"markdown","metadata":{},"source":["We now write it out to a csv file:"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["death_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"death_wacc_data\")\n","death_cause_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"death_cause_wacc_data\")"]},{"cell_type":"markdown","metadata":{},"source":["And again upload it to storage so we can reuse it: "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["%%bash\n","hdfs dfs -copyToLocal /user/root/death_wacc_data death_wacc_data\n","hdfs dfs -copyToLocal /user/root/death_cause_wacc_data death_cause_wacc_data\n","dx upload death_wacc_data/*.csv --dest users/Rosemary_Walmsley32/data/death_wacc_data.csv\n","dx upload death_cause_wacc_data/*.csv --dest users/Rosemary_Walmsley32/data/death_cause_wacc_data.csv"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"vscode":{"interpreter":{"hash":"fbf01a0adb9bb315a48b04ad23dc43d230ab447260c5925410de5a91a9cd28c7"}}},"nbformat":4,"nbformat_minor":5}
