{"metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.6.5", "mimetype": "text/x-python", "codemirror_mode": {"name": "ipython", "version": 3}, "pygments_lexer": "ipython3", "nbconvert_exporter": "python", "file_extension": ".py"}, "vscode": {"interpreter": {"hash": "fbf01a0adb9bb315a48b04ad23dc43d230ab447260c5925410de5a91a9cd28c7"}}}, "nbformat_minor": 5, "nbformat": 4, "cells": [{"cell_type": "markdown", "source": "# Extracting data using the UKB RAP\n\n## Introduction\nIn this notebook, we will dispense data for analysis using the RAP. \n\n## About this notebook\n\nThis is a Python notebook.\n\nData on the RAP is dispensed using Spark (a data processing framework well-suited to big data which works by distributing data and data processing over several machines). We will access the data using [pyspark](https://pypi.org/project/pyspark/), a Python interface to Spark.\n\nHere, we will also do some manipulation of the data using pyspark. There are alternatives: you could convert earlier to a [pandas](https://pandas.pydata.org/) or [koalas](https://koalas.readthedocs.io/en/latest/) data frame and manipulate those. Be aware that working in pandas no longer takes advantage of the distributed aspect of Spark, so you may run into issues with the size of the data. koalas looks and feels like pandas but makes use of the distributed aspect of Spark.\n\n## How to run this notebook\n\nThis notebook should be run in a *Spark in JupyterLab* session. Read how to set it up [here](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data).\n\n## Set up the session\n\nWe load modules we'll use, and set up the Spark session:", "metadata": {}}, {"cell_type": "code", "source": "import pyspark\nimport dxpy # tools starting with 'dx' are from the DNANexus ecosystem\nimport dxdata\nfrom pyspark.sql.functions import when, concat_ws\nfrom re import sub", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "code", "source": "sc = pyspark.SparkContext()\nspark = pyspark.sql.SparkSession(sc)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Dispense a dataset", "metadata": {}}, {"cell_type": "code", "source": "dispensed_database = dxpy.find_one_data_object(\n    classname=\"database\", \n    name=\"app*\", \n    folder=\"/\", \n    name_mode=\"glob\", \n    describe=True)\ndispensed_database_name = dispensed_database[\"describe\"][\"name\"]\n\ndispensed_dataset = dxpy.find_one_data_object(\n    typename=\"Dataset\", \n    name=\"app*.dataset\", \n    folder=\"/\", \n    name_mode=\"glob\")\ndispensed_dataset_id = dispensed_dataset[\"id\"]", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Load dataset", "metadata": {}}, {"cell_type": "code", "source": "dataset = dxdata.load_dataset(id=dispensed_dataset_id)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Tabular participant data\n\nWe first load the 'straightforward' tabular participant data:", "metadata": {}}, {"cell_type": "code", "source": "participant = dataset[\"participant\"]", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Next, we'll load a list of fields we might want for our analysis. We use this utility function:", "metadata": {}}, {"cell_type": "code", "source": "def load_column_list(file_name):\n    \"\"\"Load list of UK Biobank column IDs from file\n    Column IDs can be obtained from the UK Biobank showcase: https://biobank.ndph.ox.ac.uk/showcase/index.cgi\n    e.g. 90012 refers to the recommended variable for accelerometer measured physical activity\n        https://biobank.ndph.ox.ac.uk/showcase/field.cgi?id=90012\n        \n    This function is due to Aiden Doherty.\n\n    :param str file_name: Name of file listing UK Biobank column IDs\n    :return: list of IDs\n    :rtype: list\n    :Example:\n    >>> load_column_list(\"field_list.txt\")\n    [\"31\", \"34\", \"52\" ...]\n    \"\"\"\n    \n    column_IDs = []\n    \n    with open(file_name) as f:\n            for line in f:\n                li = line.strip()\n                if \"#\" in li:\n                    li = li.split(\"#\")[0].strip()\n                if not li.startswith(\"#\") and not li==\"\":\n                    column_IDs += [li]\n    return column_IDs", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Read in the list:", "metadata": {}}, {"cell_type": "code", "source": "column_list = load_column_list(\"/mnt/project/Rosemary_Webinar/candidate_field_list.txt\") # This is a text file listing the field IDs we'll use\n# You should change this file location as appropriate", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Next, we'll use these field numbers to work out the [field *names*](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database) we need.\n\nIn particular, UK Biobank fields sometimes have *instance* and *array* indices, which are [indicated in the field names](https://dnanexus.gitbook.io/uk-biobank-rap/frequently-asked-questions#how-are-column-names-determined-for-the-dispensed-database). Instances refer to different occasions when participants were measured (e.g. different visits to the assessment centre). For most instanced variables, [instance 0 is baseline, instance 1 is a resurvey among a limited number of participants, instance 2 is the imaging visit, and instance 3 is the first repeat imaging visit](https://biobank.ndph.ox.ac.uk/ukb/instance.cgi?id=2). At each instance after baseline, only a subset of participants were measured. Array indices are used where variables have multiple items e.g. participants gave multiple answers (e.g. employment history) or had multiple measurements on a single occasion (e.g. blood pressure).\n\nWe write utility functions to get names for a particular field and instance. These functions are based on [functions from DNANexus available on GitHub here](https://github.com/dnanexus/OpenBio/blob/master/UKB_notebooks/ukb-rap-pheno-basic.ipynb). ", "metadata": {}}, {"cell_type": "code", "source": "def fields_for_id(field_id):\n    from distutils.version import LooseVersion\n    field_id = str(field_id)\n    fields = participant.find_fields(name_regex=r'^p{}(_i\\d+)?(_a\\d+)?$'.format(field_id))\n    return sorted(fields, key=lambda f: LooseVersion(f.name))\n\ndef field_names_for_id_instanced(field_id, instance=\"i0\", include_non_instanced=True):\n    candidate_fields = [f.name for f in fields_for_id(field_id)]\n    return_fields = [f for f in candidate_fields if instance in f]\n    if (include_non_instanced): # This means fields without instancing (e.g. sex) will be included, and defaults to true\n        return_fields += [f for f in candidate_fields if \"_i\" not in f]\n    return return_fields", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "In this analysis, we will take non-accelerometer participant data from baseline. This means we need variables from instance 0 or with no instancing (some variables, like sex, have one value across all visits; the accelerometer variables are also not instanced):", "metadata": {}}, {"cell_type": "code", "source": "field_list = [\"eid\"]\nfor col in column_list:\n    field_list += field_names_for_id_instanced(col, instance = \"i0\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "An aside: as accelerometer wear occurred several years after baseline assessment, and some participants have undergone repeat assessments between baseline assessment and accelerometer wear, so we might alternatively use data from the most recent assessment before accelerometer wear for covariate adjustment. Given only a small proportion of participants have a second visit before accelerometer wear, for simplicity we ignore that here. However, we will add a couple of fields on self-reported conditions from repeat assessments, so we can more rigorously account for prevalent disease.", "metadata": {}}, {"cell_type": "code", "source": "field_list += [\"p6150_i1\", \"p6150_i2\", \"p53_i1\", \"p53_i2\"] # we'll take a couple of extra fields to allow us to\n# more rigorously exclude people with prevalent disease", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "If we just input the field list like this, we'll retrieve fields with their UK Biobank field names (format: [pX_iX_aX](https://dnanexus.gitbook.io/uk-biobank-rap/working-on-the-research-analysis-platform/using-spark-to-analyze-tabular-data#database-columns)). We can set up some aliases with more human readable names:\n\n[We use two different approaches here to illustrate them- you can use either, neither (i.e. use the fields with their original field names), or a mix.]", "metadata": {}}, {"cell_type": "code", "source": "# We'll handcraft some of the names to slot neatly into later code\nfield_list_aliases = {\n    \"eid\" : \"eid\", \n    \"p31\" :  \"sex\",\n    \"p52\" : \"month_birth\", \n    \"p34\" : \"year_birth\", \n    \"p54_i0\" :\"ukb_assess_cent\", \n    \"p21000_i0\" : \"ethnicity_raw\",\n    \"p189\" : \"tdi_raw\",\n    \"p6138_i0\" : \"qualif_raw\",\n    \"p845_i0\" : \"age_education_raw\",\n    \"p20116_i0\" :\"smoking_raw\",\n    \"p1558_i0\" :\"alcohol_raw\", \n    \"p21001_i0\" : \"BMI_raw\", \n    \"p191\": \"date_lost_followup\",\n    \"p6150_i0\": \"self_report_cvd_baseline\", \n    \"p6150_i1\": \"self_report_cvd_inst_1\", \n    \"p6150_i2\": \"self_report_cvd_inst_2\", \n    \"p53_i0\": \"date_baseline\", \n    \"p53_i1\": \"date_inst_1\", \n    \"p53_i2\": \"date_inst_2\", \n    \"p90016\" : \"quality_good_calibration\",\n    \"p90183\" : \"clips_before_cal\",\n    \"p90185\" : \"clips_after_cal\", \n    \"p90187\" : \"total_reads\",\n    \"p90015\" :\"quality_good_wear_time\",\n    \"p90012\": \"overall_activity\", \n    \"p90011\": \"date_end_accel\"\n    }\n\n# We'll then auto-add names for other fields \ndataset_fields = [participant.find_field(name=x) for x in field_list] # get the full field info, not just the name\nfor field in dataset_fields:\n    if field.name not in field_list_aliases.keys():\n        field_list_aliases[field.name] = field.title", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We retrieve relevant data:", "metadata": {}}, {"cell_type": "code", "source": "participant_data = participant.retrieve_fields(names=field_list, engine=dxdata.connect(), coding_values=\"replace\", column_aliases=field_list_aliases)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Setting coding values to \"replace\" in this function means we get variables' values (rather than the coded format they are stored in).", "metadata": {}}, {"cell_type": "markdown", "source": "As we will analyse only those participants with accelerometer data, we drop participants without accelerometer data:", "metadata": {}}, {"cell_type": "code", "source": "participant_wacc_data = participant_data.filter(participant_data.date_end_accel.isNotNull()) # the end time of accelerometer wear should be present for all\n# participants with accelerometer data ", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We now write the data out as a csv file. \n\nVariables for which the value is an array (the usual format where the participant could tick several boxes) cannot be written to csv. We convert the affected variables to string variables: ", "metadata": {}}, {"cell_type": "code", "source": "arrayed_cols = [\"qualif_raw\", \"self_report_cvd_baseline\", \"self_report_cvd_inst_1\", \"self_report_cvd_inst_2\"]\nfor col in arrayed_cols:\n    participant_wacc_data = participant_wacc_data.withColumn(col, concat_ws(\"|\", col))", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Spark stores and works with the data in several parts, so we coalesce them into one part:", "metadata": {}}, {"cell_type": "code", "source": "participant_wacc_data = participant_wacc_data.coalesce(1)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We then set options and write to a csv file:", "metadata": {}}, {"cell_type": "code", "source": "participant_wacc_data.write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"participant_wacc_data\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "Currently it is saved only in this session, so we need to upload to storage for reuse. As it was written by Spark, it is also stored in the 'hdfs' (Hadoop Distributed File System), in a structured folder, so we first get it out of that and into 'standard' storage:", "metadata": {}}, {"cell_type": "code", "source": "%%bash\nhdfs dfs -copyToLocal /user/root/participant_wacc_data participant_wacc_data # this gets it out of the hdfs\ndx upload participant_wacc_data/*.csv --dest Accelerometry_RAP_Demo/participant_wacc_data.csv # this uploads to the permanent storage on the RAP", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "## Hospital Episode Statistics", "metadata": {}}, {"cell_type": "markdown", "source": "There are several sources of data on health and disease outcomes in the UK Biobank dataset. These include 'Hospital Episode Statistics' (i.e. data from inpatient hospital admissions), death registry data, cancer registry data, primary care data on some participants/for some purposes, and some outcomes added to the tabular data (including the 'algorithmically defined outcomes' identifying particular diseases across different data sources). \n\nFor this analysis (following [1](https://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1003487)), we will use Hospital Episode Statistics data on diagnoses and death data.\n\nWe will use two aspects of the Hospital Episode Statistics data: hesin and hesin_diag. An entry in hesin is associated with a single hospital *episode* (generally: [\"a continuous period\nof admitted patient care administered under one consultant within that one hospital provider\"](https://biobank.ndph.ox.ac.uk/ukb/ukb/docs/HospitalEpisodeStatistics.pdf)), and provides information such as dates associated with the episode. Entries in hesin_diag are associated with a particular diagnosis within a particular episode.\n\nIn this section, we will access, process, and save it.\n\nLoad the data: ", "metadata": {}}, {"cell_type": "code", "source": "hesin = dataset[\"hesin\"]\nhesin_diag = dataset[\"hesin_diag\"]\n\nhesin_data = hesin.retrieve_fields(engine=dxdata.connect())\nhesin_diag_data = hesin_diag.retrieve_fields(engine=dxdata.connect())", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "A small proportion of episodes are missing an episode date but have different dates associated with the episode. In an optional additional step, we make a new 'dateepiimp' column which takes the value of 'epistart', but when that is missing imputes them with 'disdate':", "metadata": {}}, {"cell_type": "code", "source": "hesin_data = hesin_data.withColumn(\"dateepiimp\",\n                                   when(hesin_data[\"epistart\"].isNotNull(), hesin_data[\"epistart\"]).otherwise(hesin_data[\"disdate\"]))", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": " To associate dates with diagnoses, we need to join hesin and hesin_diag. We join on 'eid' (participant ID), 'ins_index' (the hospital episode in the data) and 'dnx_hesin_id' (which is a combination of the participant ID and the instance index and so can be used to join uniquely - we join on all three just to avoid duplicating columns). ", "metadata": {}}, {"cell_type": "code", "source": "hes_data = hesin_data.join(\n    hesin_diag_data, \n    [\"eid\", \"ins_index\", \"dnx_hesin_id\"],\n    \"left_outer\"\n)", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We keep only those participants with accelerometry data:", "metadata": {}}, {"cell_type": "code", "source": "wacc_data =  participant_wacc_data.select(\"eid\").rdd.flatMap(lambda x: x).collect()\nhes_wacc_data = hes_data.filter(hes_data.eid.isin(wacc_data))", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "As for the tabular participant data, we coalesce to a single part and write out to a csv file:", "metadata": {}}, {"cell_type": "code", "source": "hes_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"hes_wacc_data\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "And again upload it to storage so we can reuse it: ", "metadata": {}}, {"cell_type": "code", "source": "%%bash\nhdfs dfs -copyToLocal /user/root/hes_wacc_data hes_wacc_data\ndx upload hes_wacc_data/*.csv --dest Accelerometry_RAP_Demo/hes_wacc_data.csv", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "# Death data", "metadata": {}}, {"cell_type": "code", "source": "death = dataset[\"death\"]\ndeath_cause = dataset[\"death_cause\"]\n\ndeath_data = death.retrieve_fields(engine=dxdata.connect())\ndeath_cause_data = death_cause.retrieve_fields(engine=dxdata.connect())", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "To reduce the size of the dataset, we keep only those participants with accelerometry data:", "metadata": {}}, {"cell_type": "code", "source": "death_wacc_data = death_data.filter(death_data.eid.isin(wacc_data))\ndeath_cause_wacc_data = death_cause_data.filter(death_cause_data.eid.isin(wacc_data))", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "We now write it out to a 'csv' file", "metadata": {}}, {"cell_type": "code", "source": "death_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"death_wacc_data\")\ndeath_cause_wacc_data.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(\"death_cause_wacc_data\")", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}, {"cell_type": "markdown", "source": "And again upload it to storage so we can reuse it: ", "metadata": {}}, {"cell_type": "code", "source": "%%bash\nhdfs dfs -copyToLocal /user/root/death_wacc_data death_wacc_data\nhdfs dfs -copyToLocal /user/root/death_cause_wacc_data death_cause_wacc_data\ndx upload death_wacc_data/*.csv --dest Accelerometry_RAP_Demo/death_wacc_data.csv\ndx upload death_cause_wacc_data/*.csv --dest Accelerometry_RAP_Demo/death_cause_wacc_data.csv", "metadata": {"trusted": true}, "execution_count": null, "outputs": []}]}